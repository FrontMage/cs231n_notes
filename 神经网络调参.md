# 神经网络调参

本文主要总结自斯坦福cs231n课程，尽量从数学的角度给出为何如此调整以及如何调整超参数。

## 激发函数

![sigmoid](sigmoid.jpeg)

`sigmoid`是一个常用的线性到非线性转换的函数，这个函数作为激发函数有两个主要的缺点：

1. 当sigmoid的值接近0和1时，函数会变得越来越平行于x轴。这意味着在0和1附近，函数的导数趋近于0，这会导致反向传播的梯度下降算法失效。试想，来自上游的梯度虽然有值，但是自身的梯度趋近于零，于是无论上游如何优化，自身很难把这个优化反向传播下去。

2. sigmoid不是以0为中心的函数，实际上它永远大于0，这个会导致它的下一层的输入全是正数。由于f=wx+b，x是上一层的输入，它全大于0，所以反向传播时，w要么全大于0，要么全小于0，取决于上游的梯度的符号。不过由于大部分梯度下降的算法是批量求梯度然后相加的，所以有正有负，有时候可以抵消这个问题。

![tanh](tanh.jpeg)

`tanh`是sigmoid的一个变种，把sigmoid的缩放了一下使得它在0点对称，所以tanh解决了第二个问题，没有解决第一个问题。

![relu](relu.jpeg)

`relu`的特点很鲜明，就是过了0点就激发，否则就不激发，由于它的计算比sigmoid简单太多，所以实际上它的收敛速度比sigmoid快很多。

不过relu还是有它的问题，当上游的梯度超级大的时候，肯能会让relu的值偏移到0点左边的地方，这样以后无论上游如何优化，流经这个relu的梯度因为relu在这一段的梯度是0，更新的梯度也永远是0，这是所谓的`relu死亡`问题。

为了解决这个问题，`Leaky ReLUs`被提出了，它和relu的区别是，左边的水平直线会有一个非常小的斜率，例如0.01，这样就算进入了左边的死亡区域，也能回来。

另一个综合了relu和leaky relu的激发函数叫做`maxout`，不过maxout的参数数量会是relu和leaky relu的两倍，虽然它的效果可以说是最好的，但是计算成本会更高。

```
总结来说，选relu，可以尝试tanh，但是tanh所取得的成果会比relu系列更差一点。
```

